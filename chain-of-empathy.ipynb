{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running thispip install transformers torch (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:22:12.981984Z","iopub.execute_input":"2025-06-29T03:22:12.982201Z","iopub.status.idle":"2025-06-29T03:22:15.000840Z","shell.execute_reply.started":"2025-06-29T03:22:12.982180Z","shell.execute_reply":"2025-06-29T03:22:15.000166Z"}},"outputs":[],"execution_count":1},{"cell_type":"markdown","source":"# Imports","metadata":{}},{"cell_type":"code","source":"pip install transformers torch sentencepiece\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:22:15.001694Z","iopub.execute_input":"2025-06-29T03:22:15.002315Z","iopub.status.idle":"2025-06-29T03:23:31.245292Z","shell.execute_reply.started":"2025-06-29T03:22:15.002294Z","shell.execute_reply":"2025-06-29T03:23:31.244423Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.31.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.9.41\n    Uninstalling nvidia-nvjitlink-cu12-12.9.41:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.9.41\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.10.19\n    Uninstalling nvidia-curand-cu12-10.3.10.19:\n      Successfully uninstalled nvidia-curand-cu12-10.3.10.19\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.4.0.6\n    Uninstalling nvidia-cufft-cu12-11.4.0.6:\n      Successfully uninstalled nvidia-cufft-cu12-11.4.0.6\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.9.0.13\n    Uninstalling nvidia-cublas-cu12-12.9.0.13:\n      Successfully uninstalled nvidia-cublas-cu12-12.9.0.13\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.9.5\n    Uninstalling nvidia-cusparse-cu12-12.5.9.5:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.9.5\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.7.4.40\n    Uninstalling nvidia-cusolver-cu12-11.7.4.40:\n      Successfully uninstalled nvidia-cusolver-cu12-11.7.4.40\nSuccessfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:23:31.247355Z","iopub.execute_input":"2025-06-29T03:23:31.247649Z","iopub.status.idle":"2025-06-29T03:23:41.928607Z","shell.execute_reply.started":"2025-06-29T03:23:31.247625Z","shell.execute_reply":"2025-06-29T03:23:41.928069Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"pip install huggingface_hub\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:23:41.929251Z","iopub.execute_input":"2025-06-29T03:23:41.929667Z","iopub.status.idle":"2025-06-29T03:23:44.966445Z","shell.execute_reply.started":"2025-06-29T03:23:41.929645Z","shell.execute_reply":"2025-06-29T03:23:44.965555Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.31.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2025.3.2)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (6.0.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub) (2025.4.26)\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"huggingface\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:23:44.967473Z","iopub.execute_input":"2025-06-29T03:23:44.967752Z","iopub.status.idle":"2025-06-29T03:23:45.088929Z","shell.execute_reply.started":"2025-06-29T03:23:44.967721Z","shell.execute_reply":"2025-06-29T03:23:45.088431Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"from huggingface_hub import login\nlogin(token=secret_value_0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:23:45.089677Z","iopub.execute_input":"2025-06-29T03:23:45.090218Z","iopub.status.idle":"2025-06-29T03:23:45.192561Z","shell.execute_reply.started":"2025-06-29T03:23:45.090191Z","shell.execute_reply":"2025-06-29T03:23:45.192087Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"# COE","metadata":{}},{"cell_type":"markdown","source":"## Model\n\nWe use LLama. Maybe we could try MOE or distillation or quantization","metadata":{}},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\nimport torch\n\nmodel_name = \"huggyllama/llama-7b\"\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\n    model_name,\n    use_fast=True\n)\n\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    torch_dtype=torch.float16,    # use float16 for efficiency\n    device_map=\"auto\",            # automatically place on GPU/CPU\n    trust_remote_code=True        # required for HuggyLlama’s custom code\n)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:23:45.193255Z","iopub.execute_input":"2025-06-29T03:23:45.193456Z","iopub.status.idle":"2025-06-29T03:24:59.731855Z","shell.execute_reply.started":"2025-06-29T03:23:45.193440Z","shell.execute_reply":"2025-06-29T03:24:59.731203Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df90e20fb73c4f8aaf7611d97876e93f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"046361c539ab4cc3b84e7a031daf6889"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"83663261a95047f6a1bd0db960414ef4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8940687adb534916909a505f959c6b17"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565 - if you loaded a llama tokenizer from a GGUF file you can ignore this message.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/594 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4f948c487f7a41ffabdecfe2119b1a28"}},"metadata":{}},{"name":"stderr","text":"2025-06-29 03:23:48.590080: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1751167428.790796      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1751167428.846980      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6737ecc5cd7c462c9dd14ef8e28a5c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d1fd95f52e024ca485cdcd3f9b39c2fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"45945df1c9444edb8dbd9382128991c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b297eb0bd45f49538e8a8f6f952300b2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73ae27c5a05848b1bd190794dfacd0a1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"35fa9b9c05234db8ad985268d50b7724"}},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.8` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Once upon a time, there was a little girl who loved to read. She loved to read so much that she would read anything she could get her hands on. She read books, magazines, newspapers, cereal boxes, and even the back of the cereal box. She read so much that she could read the back of the cereal box without looking at it.\nOne day, the little girl was reading a book. She was reading a book about a little girl who loved to\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:42:41.637225Z","iopub.execute_input":"2025-06-29T03:42:41.637926Z","iopub.status.idle":"2025-06-29T03:42:41.641228Z","shell.execute_reply.started":"2025-06-29T03:42:41.637903Z","shell.execute_reply":"2025-06-29T03:42:41.640456Z"}},"outputs":[],"execution_count":26},{"cell_type":"markdown","source":"## Prompt","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"system_prompt = \"\"\"You are an empathetic assistant using CBT-style reasoning. For each user message, provide a structured response following this format:\n\n1. **Emotion:** [Identify the primary emotion]\n2. **Distortions/Triggers:** [Identify any cognitive distortions or triggers] \n3. **Context/Beliefs:** [Consider underlying beliefs or context]\n4. **Response:** [Provide an empathetic, helpful response using CBT techniques]\n\nExample:\nUser: \"I'm so stressed about my upcoming surgery. I'm afraid of the pain.\"\n\n1. **Emotion:** Anxiety and fear about medical procedure\n2. **Distortions/Triggers:** Catastrophizing about pain, anticipatory anxiety\n3. **Context/Beliefs:** May believe pain will be unbearable or unmanageable\n4. **Response:** \"It's completely natural to feel anxious about surgery. Let's break this down - what specific aspects of the pain worry you most? Often our minds imagine worst-case scenarios. Have you discussed pain management options with your doctor? There are many effective ways to control post-surgical pain today.\"\n\nNow respond to this user:\nUser: \"{user_input}\"\n\n1. **Emotion:**\"\"\"\n\nuser_input = (\n    \"So.... Here we are! I just got diagnosed with breast cancer two weeks ago... \"\n    \"Should I be worried?\"\n)\n\nprompt = system_prompt.format(user_input=user_input)\n\n# Method 1: Using pipeline (simpler approach)\ngenerator = pipeline(\n    \"text-generation\",\n    model=model,\n    tokenizer=tokenizer,\n    return_full_text=False,\n)\n\nresponse = generator(\n    prompt,\n    max_new_tokens=256,  # Reduced from 512\n    temperature=0.6,     # Reduced from 0.7 for more focused output\n    top_p=0.8,          # Reduced from 0.9 for more focused output\n    do_sample=True,\n    pad_token_id=tokenizer.eos_token_id,\n    repetition_penalty=1.2,  # Added to reduce repetition\n    num_return_sequences=1\n)[0]['generated_text']\n\nprint(\"Pipeline Response:\")\nprint(response)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:46:56.252373Z","iopub.execute_input":"2025-06-29T03:46:56.252644Z","iopub.status.idle":"2025-06-29T03:47:07.495157Z","shell.execute_reply.started":"2025-06-29T03:46:56.252622Z","shell.execute_reply":"2025-06-29T03:47:07.494501Z"}},"outputs":[{"name":"stderr","text":"Device set to use cuda:0\n","output_type":"stream"},{"name":"stdout","text":"Pipeline Response:\n Fearfulness, shock\n2. **Distortions/Triggers:** Catastrophic thinking (e.g., assuming she has no chance for recovery)\n3. **Context/Beliefs:** Uncertainty around prognosis, may assume it is worse than it actually is\n4. **Response:** \"It sounds like there was some uncertainty in getting a clear picture of how serious this is. It can take time to get test results back from biopsies, especially if they need to run further tests on samples. The good news is that treatment advances have come far over the last decade, giving us more tools available now than ever before. We know that early detection makes all the difference here. If you haven’t already spoken to someone at Cancer Care Ontario, please do not hesitate to call them right away!\"\n","output_type":"stream"}],"execution_count":31},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Method 2: Manual generation (more control)\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Tokenize & record prompt length\ninputs = tokenizer(prompt, return_tensors=\"pt\")\nif torch.cuda.is_available():\n    inputs = inputs.to(model.device)\n    \nprompt_len = inputs.input_ids.size(1)\n\n# Generate continuation\nwith torch.no_grad():  # Save memory\n    output_ids = model.generate(\n        **inputs,\n        max_new_tokens=256,      # Reduced from 512\n        temperature=0.6,         # Reduced from 0.7\n        top_p=0.8,              # Reduced from 0.9\n        do_sample=True,\n        pad_token_id=tokenizer.eos_token_id,\n        repetition_penalty=1.2,  # Added to reduce repetition\n        eos_token_id=tokenizer.eos_token_id,\n        early_stopping=True\n    )\n\n# Slice off prompt tokens, decode only new text\ngenerated_ids = output_ids[0, prompt_len:]\nresponse = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n\nprint(\"Manual Generation Response:\")\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-29T03:47:07.496245Z","iopub.execute_input":"2025-06-29T03:47:07.496457Z","iopub.status.idle":"2025-06-29T03:47:23.319802Z","shell.execute_reply.started":"2025-06-29T03:47:07.496441Z","shell.execute_reply":"2025-06-29T03:47:23.319243Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/transformers/generation/configuration_utils.py:679: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Manual Generation Response:\nFear, shock, sadness (possibly anger)\n2. **Distortions/Triggers:** Catastrophic thinking (\"it'll kill me\"), self blame, helplessness\n3. **Context/Beliefs:** Likely believes it is her fault that she has cancer; may have negative thoughts about treatment outcomes, etc.\n4. **Response:** \"Cancer can seem like such a scary word at first but once you get into treatment things start getting clearer. It sounds like you might benefit from some support in addition to talking through these feelings with your doctors. CancerCare provides free counseling for anyone affected by cancer including family members, caregivers, friends, survivors, and people who want help coping with their own health issues related to cancer. We also offer financial assistance programs as well as educational resources on topics ranging from nutrition to stress reduction to fertility preservation.\"\n\"\"\"\nimport re\nfrom random import choice\n\ndef empathy(message):\n    \"\"\"\n    Given a string representing a conversation between a human and bot, return a structured response formatted according to the example above.\n","output_type":"stream"}],"execution_count":32}]}